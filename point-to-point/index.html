<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Point-to-Point revisits point-based motion representation, which requires no user effort, provides accurate motion information, and is adaptable enough to be readily applied to various subjects.">
  <meta name="keywords" content="Diffusion, Video Editing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Point-to-Point</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Hi+Melody&family=Nanum+Myeongjo:wght@700&display=swap" rel="stylesheet">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Point-to-Point: Sparse Motion Guidance for Controllable Video Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ldynx.github.io/">Yeji Song</a>,
            </span>
            <span class="author-block">
              Jaehyun Lee,
            </span>
            <span class="author-block">
              Mijin Koo,
            </span>
            <span class="author-block">
              <a href="https://junhoo-lee.com">JunHoo Lee</a>,
            </span>
            <span class="author-block">
              <a href="http://mipal.snu.ac.kr/index.php/Main_Page">Nojun Kwak</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Seoul National University</span>
          </div>

          <!-- <div class="is-size-4"><b> ECCV 2024 </b></div> -->
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.02503"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.02503"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ldynx/SAVE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="video-row" style="display: flex; gap: 16px;">
          <figure style="text-align: center;">
            <video poster="" id="cat-sun" autoplay controls muted loop playsinline height="70%">
              <source src="./static/videos/1-2_cat_in_the_sun/source.mp4" type="video/mp4">
            </video>
            <figcaption>Cat (Source)</figcaption>
          </figure>
          <figure style="text-align: center;">
            <video poster="" id="fox-sun" autoplay controls muted loop playsinline height="70%">
              <source src="./static/videos/1-2_cat_in_the_sun/cat_to_fox_ours.mp4" type="video/mp4">
            </video>
            <figcaption>Fox (Edit)</figcaption>
          </figure>
        </div>

        <div class="video-row" style="display: flex; gap: 16px;">
          <figure style="text-align: center;">
            <video poster="" id="cats" autoplay controls muted loop playsinline height="70%">
              <source src="./static/videos/3-1_two_cats/source.mp4" type="video/mp4">
            </video>
            <figcaption>Cats (Source)</figcaption>
          </figure>
          <figure style="text-align: center;">
            <video poster="" id="pandas" autoplay controls muted loop playsinline height="70%">
              <source src="./static/videos/3-1_two_cats/cats_to_red_pandas_ours.mp4" type="video/mp4">
            </video>
            <figcaption>Red pandas (Edit)</figcaption>
          </figure>
        </div>

        <div class="video-row" style="display: flex; gap: 16px;">
          <figure style="text-align: center;">
            <video poster="" id="woman" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/4-1_woman_boxing/source.mp4" type="video/mp4">
            </video>
            <figcaption>Woman (Source)</figcaption>
          </figure>
          <figure style="text-align: center;">
            <video poster="" id="batman" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/4-1_woman_boxing/woman_to_batman_ours.mp4" type="video/mp4">
            </video>
            <figcaption>Batman (Edit)</figcaption>
          </figure>
        </div>

        <div class="video-row" style="display: flex; gap: 16px;">
          <figure style="text-align: center;">
            <video poster="" id="monkey" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/4-2_monkey_eating/source.mp4" type="video/mp4">
            </video>
            <figcaption>Monkey (Source)</figcaption>
          </figure>
          <figure style="text-align: center;">
            <video poster="" id="otter" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/4-2_monkey_eating/monkey_to_otter_ours.mp4" type="video/mp4">
            </video>
            <figcaption>Otter (Edit)</figcaption>
          </figure>
        </div>

      </div>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fifth-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. 
            Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations 
            that are either overfitted to the layout or only implicitly defined. To overcome this limitation, 
            we revisit point-based motion representation. However, identifying meaningful points remains challenging without
            human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, 
            anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. 
            Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly
            relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. 
            Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, 
            achieving superior performance in terms of edit and motion fidelity.
          </p>
          <br/>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Motivation -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fifth-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Signal-based methods extract and leverage an explicit motion signal from the source video, 
            whereas adaptation-based methods embed motion implicitly in the model or latent space,
            optimizing those representations on the source video. 
            Point-based methods guide the motion using semantic points and their trajectory. 
            Despite their respective strengths, all three approaches struggle to satisfy the requirements above. 
            Signal-based methods overfit to the spatial layout of the source video, often compromising edit fidelity. 
            Adaptation-based methods often struggle to capture precise motion dynamics, leading to low motion fidelity.
            Point-based methods require manual annotations, resulting in degraded edit and motion fidelity when points are inaccurate.
            To address these challenges, we propose novel, fully-automated and motion-aligned points.
          </p>
          <br/>
        </div>
        <figure class="video-row-content" style="display: flex; flex-direction: row; gap: 16px;">
          <div style="flex: 1; text-align: center;">
            <video poster="" id="motivation" autoplay controls muted loop playsinline>
              <source src="./static/videos/1-1_kitten_walk/source.mp4" type="video/mp4">
            </video>
            <figcaption>Kitten (Source)</figcaption>
          </div>
          <div style="flex: 1; text-align: center;">
            <div style="margin-bottom: 4px; font-weight: bold;">Ours</div>
            <video poster="" id="motivation" autoplay controls muted loop playsinline>
              <source src="./static/videos/1-1_kitten_walk/kitten_to_rabbit_ours.mp4" type="video/mp4">
            </video>
          </div>
          <div style="flex: 1; text-align: center;">
            <div style="margin-bottom: 4px; font-weight: bold;">Signal-based<sup>[1]</sup></div>
            <video poster="" id="motivation" autoplay controls muted loop playsinline>
              <source src="./static/videos/1-1_kitten_walk/kitten_to_rabbit_video-grain.mp4" type="video/mp4">
            </video>
            <figcaption>Rabbit (Edit)</figcaption>
          </div>
          <div style="flex: 1; text-align: center;">
            <div style="margin-bottom: 4px; font-weight: bold;">Adaptation-based<sup>[2]</sup></div>
            <video poster="" id="motivation" autoplay controls muted loop playsinline>
              <source src="./static/videos/1-1_kitten_walk/kitten_to_rabbit_dmt.mp4" type="video/mp4">
            </video>
          </div>
        </figure>
      </div>
    </div>
    <!--/ Motivation -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-fifth-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Anchor Token Selection</h3>
          <p>
            First, we define <b>motion tokens</b>, which are a set of extracted features using a video diffusion model. 
            In contrast to prior works (Geyer et al. 2024; Wang et al. 2024a), where features (often referred to as â€œtokens") are independently extracted
            from each latent pixel in every frame, we track those tokens across frames and obtain distinct motion trajectories to extract
            more representative features of motion dynamics. 
            From this set, we select a subset of anchor tokens that represent the most informative motion patterns in the video. 
            Each anchor token captures a distinct local motion, and together they form a comprehensive summary of the overall motion dynamics.
          </p>
          <h3 class="title is-4">Anchor Token Alignment</h3>
          <p>
            Meanwhile, the new subject in the edited video may not align well with the anchor tokens extracted from the source video. 
            To address this, we <b>adjust the anchor tokens</b> to better match the new layout by matching each anchor token to the 
            motion tokens in the edited video whose feature is most similar. Then, we relocate each anchor token to its corresponding position.
          </p>
          <br/>
          <div class="column is-centered has-text-centered">
            <img src="static/images/method.png", width=70%>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{song2023save,
      title={SAVE: Protagonist Diversification with Structure Agnostic Video Editing}, 
      author={Yeji Song and Wonsik Shin and Junsoo Lee and Jeesoo Kim and Nojun Kwak},
      year={2023},
      eprint={2312.02503},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!--
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
